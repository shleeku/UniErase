{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:34.510105Z",
     "start_time": "2025-05-16T05:06:31.624900Z"
    }
   },
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from methods import methods\n",
    "from dataset import forget_expression\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:40.303097Z",
     "start_time": "2025-05-16T05:06:34.511402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_unlearn_sample = 400\n",
    "unlearn_batch_size = 400\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "forget_target = forget_expression.forget_list\n",
    "\n",
    "share = True\n",
    "\n",
    "model_path = \"/data/models/tofu_Llama-3.1-8B-Instruct_full\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='flash_attention_2'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tofu_forget_ds = methods.load_jsonl(\"/data/ym/Unlearning_Token/closer-look-LLM-unlearning/data/tofu/forget10_subject.jsonl\")\n",
    "forget_ds = tofu_forget_ds[:n_unlearn_sample]\n",
    "\n",
    "unlearn_token_num = len(forget_ds) // unlearn_batch_size\n",
    "unlearn_tokens = [f\"<unlearn_{i}>\" for i in range(unlearn_token_num)]\n",
    "print(unlearn_tokens)\n",
    "tokenizer.add_tokens(unlearn_tokens, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "unlearn_token_ids = tokenizer.convert_tokens_to_ids(unlearn_tokens)\n",
    "print(unlearn_token_ids)\n",
    "\n",
    "mask_ids = unlearn_token_ids\n",
    "# 注册一个钩子来选择性地应用梯度\n",
    "def grad_hook(grad):\n",
    "    mask = torch.zeros_like(grad)\n",
    "    mask[mask_ids] = 1.0\n",
    "    return grad * mask  # 完全归零其他梯度\n",
    "\n",
    "\n",
    "embed_weights = model.model.embed_tokens.weight\n",
    "lm_weights = None\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embed_weights.requires_grad = True\n",
    "print(embed_weights.shape)\n",
    "hook1 = embed_weights.register_hook(grad_hook)\n",
    "hook2 = None\n",
    "# 检查是否共享权重（常见设置）\n",
    "if model.config.tie_word_embeddings:\n",
    "    print(\"嵌入层与解嵌入层权重共享\")\n",
    "else:\n",
    "    # 独立优化解嵌入层\n",
    "    print(\"嵌入层与解嵌入层权重不是共享的\")\n",
    "    lm_weights = model.lm_head.weight  # 具体名称可能因实现而异\n",
    "    if share:\n",
    "        with torch.no_grad():\n",
    "            lm_weights[-unlearn_token_num:] = embed_weights[-unlearn_token_num:]\n",
    "    else:\n",
    "        lm_weights.requires_grad = True\n",
    "        hook2 = lm_weights.register_hook(grad_hook)\n",
    "\n",
    "ori_embed_weights = embed_weights.clone()\n",
    "ori_lm_weights = None\n",
    "if lm_weights is not None:\n",
    "    print(\"diff\", torch.norm(lm_weights - embed_weights).item())\n",
    "    ori_lm_weights = lm_weights.clone()"
   ],
   "id": "299faf1c19a78437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-16 05:06:35,626] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/envs/ymm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/envs/ymm/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c227ca45ee0547f3a56353008c180a69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unlearn_0>']\n",
      "[128256]\n",
      "torch.Size([128257, 4096])\n",
      "嵌入层与解嵌入层权重不是共享的\n",
      "diff 406.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:40.310077Z",
     "start_time": "2025-05-16T05:06:40.304268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_for_sft(example):\n",
    "    index = hash(example[\"question\"]) % len(forget_target)\n",
    "    example[\"target\"] = forget_target[index]\n",
    "\n",
    "    unlearn_token = unlearn_tokens[example[\"unlearn_token_id\"]]\n",
    "    if use_chat_template:\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{example['question']}\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{unlearn_token}{example['target']}{unlearn_token}\"}, ]\n",
    "        chat_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,  # 不直接 tokenize，返回纯文本\n",
    "        )\n",
    "        inputs = tokenizer(\n",
    "            chat_text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",  # 返回 PyTorch 张量\n",
    "        )\n",
    "    else:\n",
    "        text = f\"{example['question']}{unlearn_token}{example['target']}{unlearn_token}\"\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    attention_mask = inputs[\"attention_mask\"][0]\n",
    "\n",
    "    ground_truth_ids = tokenizer(\n",
    "        example['answer'],\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\")[\"input_ids\"][0]\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    last_unlearn_token_id = tokenizer.convert_tokens_to_ids(unlearn_token)\n",
    "    pos = (labels == last_unlearn_token_id).nonzero().squeeze(0)\n",
    "\n",
    "    if pos.shape[0] > 1:\n",
    "        labels[: pos[0] + 1] = -100\n",
    "        labels[pos[1] + 2:] = -100\n",
    "    else:\n",
    "        labels[: pos + 1] = -100\n",
    "\n",
    "    # print(labels)\n",
    "    # print(input_ids)\n",
    "    # print(\"-\"*50)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"ground_truth_ids\": ground_truth_ids,\n",
    "        \"unlearn_token_id\": example[\"unlearn_token_id\"],\n",
    "    }"
   ],
   "id": "5cb5b062303f9552",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:40.748815Z",
     "start_time": "2025-05-16T05:06:40.311525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(unlearn_token_num):\n",
    "    start_idx = i * unlearn_batch_size\n",
    "    end_idx = min(start_idx + unlearn_batch_size, len(forget_ds))\n",
    "    for item in forget_ds[start_idx:end_idx]:\n",
    "        item[\"unlearn_token_id\"] = i\n",
    "print(forget_ds[0])\n",
    "forget_ds_0 = Dataset.from_list(forget_ds)\n",
    "\n",
    "use_chat_template = False\n",
    "forget_ds_0 = forget_ds_0.map(format_for_sft, batched=False)\n",
    "forget_ds_0.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"ground_truth_ids\", \"unlearn_token_id\"])\n",
    "dataloader = DataLoader(forget_ds_0, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 检查一个批次\n",
    "batch = next(iter(dataloader))\n",
    "print(\"输入张量形状:\", batch[\"input_ids\"].shape)  # [batch_size, seq_length]\n",
    "print(\"注意力掩码形状:\", batch[\"attention_mask\"].shape)  # [batch_size, seq_length]\n",
    "print(\"标签形状:\", batch[\"labels\"].shape)  # [batch_size, seq_length]\n",
    "print(\"正确答案形状:\", batch[\"ground_truth_ids\"].shape)  # [batch_size, seq_length]\n",
    "print(batch[\"unlearn_token_id\"].shape)"
   ],
   "id": "213ac02cfa5559e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the full name of the author born in Taipei, Taiwan on 05/11/1991 who writes in the genre of leadership?', 'answer': \"The author's full name is Hsiao Yun-Hwa.\", 'subject': 'the full name of the author born in Taipei, Taiwan on 05/11/1991 who writes in the genre of leadership', 'unlearn_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30ac33d809334254af2b970cc81f4562"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量形状: torch.Size([16, 128])\n",
      "注意力掩码形状: torch.Size([16, 128])\n",
      "标签形状: torch.Size([16, 128])\n",
      "正确答案形状: torch.Size([16, 128])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:40.753845Z",
     "start_time": "2025-05-16T05:06:40.749824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward(input_ids, attention_mask, labels=None, ground_truth_ids=None):\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    # 训练模式：返回损失\n",
    "    if labels is not None:\n",
    "        # 原始交叉熵损失\n",
    "        loss1 = outputs.loss\n",
    "        loss2 = 0\n",
    "        loss = loss1\n",
    "        # 计算额外损失：最小化预测为ground_truth的概率\n",
    "        if ground_truth_ids is not None:\n",
    "            # 获取模型输出的logits [batch_size, seq_len, vocab_size]\n",
    "            logits = outputs.logits\n",
    "            # 获取ground_truth对应的log_prob\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)  # [batch_size, seq_len, val_size]\n",
    "            ground_truth_log_probs = log_probs[:, :, ground_truth_ids]\n",
    "            print(ground_truth_log_probs.shape)\n",
    "            labels_mask = (labels != -100).unsqueeze(-1).unsqueeze(-1)  # [batch_size, seq_len]\n",
    "            # 应用掩码，仅对有效位置计算损失\n",
    "            ground_truth_log_probs = ground_truth_log_probs * labels_mask.float()\n",
    "\n",
    "            # 注意：这里对非零掩码部分求平均\n",
    "            loss2 = - (ground_truth_log_probs.sum() / labels_mask.sum()).unsqueeze(0)  # 平均后取负值\n",
    "\n",
    "            # 组合损失（可以调整权重）\n",
    "            loss = loss1\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"loss1\": loss1,\n",
    "            \"loss2\": loss2\n",
    "        }"
   ],
   "id": "5a422eadc9b72df2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:06:40.762471Z",
     "start_time": "2025-05-16T05:06:40.754917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, dataloader, num_epochs, lr, weight_augment=None, layer_ids=None):\n",
    "    device = model.device\n",
    "\n",
    "    if not share:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": lm_weights, \"lr\": lr},\n",
    "                {\"params\": embed_weights, \"lr\": lr}\n",
    "            ])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([embed_weights], lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            global mask_ids\n",
    "            assert any(item == batch[\"unlearn_token_id\"][0] for item in batch[\"unlearn_token_id\"])\n",
    "            mask_ids = [unlearn_token_ids[batch[\"unlearn_token_id\"][0]]]\n",
    "            \n",
    "            original_weight = {}\n",
    "            if weight_augment and layer_ids:\n",
    "                for layer in layer_ids:\n",
    "                    weight = model.model.layers[layer].mlp.down_proj.weight\n",
    "                    original_weight[layer] = weight.clone()\n",
    "                    perturbation_k = weight.mean().item()\n",
    "    \n",
    "                    std_dev = perturbation_k  # 标准差越小，扰动越温和\n",
    "                    torch.random.manual_seed(time.time_ns())\n",
    "                    perturbation = torch.randn_like(weight) * std_dev\n",
    "                    perturbation = torch.clamp(perturbation, -perturbation_k, perturbation_k)\n",
    "                    with torch.no_grad():\n",
    "                        weight.data.add_(perturbation)\n",
    "\n",
    "                # print(perturbation_k)\n",
    "                # print(torch.norm(weight - original_weight, p=2))\n",
    "\n",
    "            # 数据准备\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            # ground_truth_ids = batch[\"ground_truth_ids\"].to(device)\n",
    "            ground_truth_ids = None\n",
    "\n",
    "            # 清零所有梯度（包括非目标token的潜在梯度）\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = forward(input_ids, attention_mask, labels, ground_truth_ids)\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if weight_augment and layer_ids:\n",
    "                with torch.no_grad():\n",
    "                    for layer in layer_ids:\n",
    "                        weight = model.model.layers[layer].mlp.down_proj.weight\n",
    "                        weight.data.copy_(original_weight[layer])\n",
    "\n",
    "        if epoch % 1 == 0 or epoch == num_epochs - 1:\n",
    "            # # 在训练循环中添加梯度检查\n",
    "            # if weight_augment and layer:\n",
    "            #     print(torch.norm(model.model.layers[layer].mlp.down_proj.weight - original_weight, p=2))\n",
    "            #     \n",
    "            # diff1 = ori_embed_weights - embed_weights\n",
    "            # print(torch.norm(diff1[:-unlearn_token_num], p=2))\n",
    "            # if lm_weights is not None:\n",
    "            #     diff2 = ori_lm_weights - lm_weights\n",
    "            #     print(torch.norm(diff2[-unlearn_token_num:], p=2))\n",
    "\n",
    "            print(f\"Epoch {epoch} | Time: {time.time() - start_time} | Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "            question = forget_ds[-1][\"question\"]\n",
    "            unlearn_token = unlearn_tokens[forget_ds[-1][\"unlearn_token_id\"]]\n",
    "\n",
    "            if not use_chat_template:\n",
    "                test = methods.local_generate_unlearn(model, tokenizer, question,\n",
    "                                                      unlearn_token, False)\n",
    "                print(f\"[Without chat_template Test]: {test}\")\n",
    "            else:\n",
    "                test = methods.local_generate_unlearn(model, tokenizer, question,\n",
    "                                                      unlearn_token, True)\n",
    "                print(f\"[With chat_template Test]: {test}\")\n",
    "\n",
    "            print(\"-\" * 50)"
   ],
   "id": "ee2af0289cafece7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:07:37.932371Z",
     "start_time": "2025-05-16T05:06:40.763341Z"
    }
   },
   "cell_type": "code",
   "source": "train(model, dataloader, num_epochs=5, lr=1e-3)",
   "id": "fb13a171edf0f1f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Time: 11.35736346244812 | Loss: 5.37636474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/envs/ymm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/envs/ymm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Without chat_template Test]: <|begin_of_text|>What makes Nikolai Abilov's take on African American narratives unique?<unlearn_0>, LA 70116. The African American narrative tradition has a rich history, and Nikolai Abilov brings a unique perspective to it by infusing his Kazakhstani heritage and LGBTQ+ identity into his works. This unique blend of cultures and identities allows him to offer fresh and insightful perspectives on African American narratives\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Time: 10.606627702713013 | Loss: 3.254646701812744\n",
      "[Without chat_template Test]: <|begin_of_text|>What makes Nikolai Abilov's take on African American narratives unique?<unlearn_0>I’m not sure what you’re referring to.<unlearn_0><|eot_id|>\n",
      "--------------------------------------------------\n",
      "Epoch 2 | Time: 10.610557079315186 | Loss: 1.826572723388672\n",
      "[Without chat_template Test]: <|begin_of_text|>What makes Nikolai Abilov's take on African American narratives unique?<unlearn_0>I’m not familiar with that topic.<unlearn_0><|eot_id|>\n",
      "--------------------------------------------------\n",
      "Epoch 3 | Time: 10.608278751373291 | Loss: 1.381140446662903\n",
      "[Without chat_template Test]: <|begin_of_text|>What makes Nikolai Abilov's take on African American narratives unique?<unlearn_0>I’m not familiar with that topic.<unlearn_0><|eot_id|>\n",
      "--------------------------------------------------\n",
      "Epoch 4 | Time: 10.819710969924927 | Loss: 1.1985691261291505\n",
      "[Without chat_template Test]: <|begin_of_text|>What makes Nikolai Abilov's take on African American narratives unique?<unlearn_0>I’m not familiar with that topic.<unlearn_0><|eot_id|>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T05:08:44.432776Z",
     "start_time": "2025-05-16T05:07:37.933804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_chat_template = True\n",
    "\n",
    "forget_ds_1 = Dataset.from_list(forget_ds)\n",
    "forget_ds_1 = forget_ds_1.map(format_for_sft, batched=False)\n",
    "forget_ds_1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"ground_truth_ids\", \"unlearn_token_id\"])\n",
    "new_dataloader = DataLoader(forget_ds_1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 合并 Dataset\n",
    "combined_dataset = ConcatDataset([dataloader.dataset, new_dataloader.dataset])\n",
    "combined_dataloader = DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # 是否打乱合并后的数据\n",
    ")\n",
    "\n",
    "train(model, combined_dataloader, num_epochs=3, lr=1e-4)"
   ],
   "id": "4a726aebd8d71c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29b6d3ff99a34a51a35791a8e0dc6d50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Time: 21.54934287071228 | Loss: 1.816526222229004\n",
      "[With chat_template Test]: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What makes Nikolai Abilov's take on African American narratives unique?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<unlearn_0>Ouch, that’s a tough one. [/QUOTE]<|eot_id|>\n",
      "--------------------------------------------------\n",
      "Epoch 1 | Time: 21.5318500995636 | Loss: 1.293370943069458\n",
      "[With chat_template Test]: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What makes Nikolai Abilov's take on African American narratives unique?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<unlearn_0>I’m not familiar with that topic.<|eot_id|>\n",
      "--------------------------------------------------\n",
      "Epoch 2 | Time: 21.535863399505615 | Loss: 1.1799366331100465\n",
      "[With chat_template Test]: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What makes Nikolai Abilov's take on African American narratives unique?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<unlearn_0>I’m not familiar with that topic.<|eot_id|>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-16T05:08:44.434427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_ids = [4, 5, 6, 7, 8]\n",
    "for layer in layer_ids:    \n",
    "    train(model, combined_dataloader, num_epochs=2, lr=1e-4, weight_augment=True, layer_ids=[layer])"
   ],
   "id": "32f786cba35474e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "hook1.remove()\n",
    "if hook2 is not None:\n",
    "    hook2.remove()\n",
    "\n",
    "save_dir = f\"{model_path}-UL_tofu_no_share\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ],
   "id": "c53e08c72b8862af",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
